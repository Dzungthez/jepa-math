## ğŸ“š So sÃ¡nh 3 hÃ m Attention Visualization

### Visual Overview:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£ visualize_attention() - 2D Heatmaps                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: layers_heads = [(0, 0), (14, 0), (27, 27)]                 â”‚
â”‚                                                                     â”‚
â”‚  Output:  [Layer 0 Head 0]  [Layer 14 Head 0]  [Layer 27 Head 27] â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚           â”‚ â–ˆâ–‘â–‘â–‘      â”‚     â”‚ â–ˆâ–‘â–‘â–‘      â”‚      â”‚ â–ˆâ–‘â–‘â–‘      â”‚      â”‚
â”‚           â”‚ â–ˆâ–ˆâ–‘â–‘      â”‚     â”‚ â–ˆâ–ˆâ–‘â–‘      â”‚      â”‚ â–ˆâ–ˆâ–‘â–‘      â”‚      â”‚
â”‚           â”‚ â–‘â–ˆâ–ˆâ–‘      â”‚     â”‚ â–ˆâ–‘â–ˆâ–ˆ      â”‚      â”‚ â–ˆâ–‘â–ˆâ–ˆ      â”‚      â”‚
â”‚           â”‚ â–‘â–‘â–ˆâ–ˆ      â”‚     â”‚ â–ˆâ–‘â–‘â–ˆ      â”‚      â”‚ â–ˆâ–‘â–‘â–ˆ      â”‚      â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                     â”‚
â”‚  Best for: Detailed analysis cá»§a specific heads                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£ visualize_attention_patterns() - 1D Bar Charts                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: layers_heads = [(0, 0), (14, 0), (27, 0)]                  â”‚
â”‚         pattern_type = 'mean' or 'specific_token'                  â”‚
â”‚                                                                     â”‚
â”‚  Output:  [L0H0 Mean]      [L14H0 Mean]     [L27H0 Mean]          â”‚
â”‚           â•‘                â•‘                â•‘                      â”‚
â”‚           â•‘ â–ˆ              â•‘ â–ˆâ–ˆâ–ˆ            â•‘ â–ˆâ–ˆâ–ˆ                  â”‚
â”‚           â•‘ â–ˆâ–ˆ             â•‘ â–ˆ              â•‘ â–ˆ                    â”‚
â”‚           â•‘ â–ˆâ–ˆâ–ˆ            â•‘ â–ˆâ–ˆ             â•‘ â–ˆâ–ˆ                   â”‚
â”‚           â•šâ•â•â•            â•šâ•â•â•            â•šâ•â•â•                     â”‚
â”‚           0 5 10 15        0 5 10 15       0 5 10 15               â”‚
â”‚                                                                     â”‚
â”‚  Best for: Finding important tokens, aggregated patterns           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£ visualize_attention_layers() - Multi-Layer Grid (Paper Style)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: layer_indices = [0, 1, 2, 9, 14, 18, 23, 27]               â”‚
â”‚         head_idx = None (average all heads)                        â”‚
â”‚                                                                     â”‚
â”‚  Output:                                                            â”‚
â”‚    [L0 Avg]    [L1 Avg]    [L2 Avg]    [L9 Avg]                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–ˆâ–‘â–‘  â”‚  â”‚ â–ˆâ–ˆâ–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  (Local pattern)   â”‚
â”‚    â”‚ â–ˆâ–ˆâ–‘â–‘  â”‚  â”‚ â–‘â–ˆâ–ˆâ–‘  â”‚  â”‚ â–‘â–ˆâ–ˆâ–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚                    â”‚
â”‚    â”‚ â–‘â–ˆâ–ˆâ–‘  â”‚  â”‚ â–‘â–‘â–ˆâ–ˆ  â”‚  â”‚ â–‘â–‘â–ˆâ–ˆ  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                     â”‚
â”‚   [L14 Avg]   [L18 Avg]   [L23 Avg]   [L27 Avg]                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  (Attention sink)  â”‚
â”‚    â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚                    â”‚
â”‚    â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚  â”‚ â–ˆâ–‘â–‘â–‘  â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                     â”‚
â”‚  Best for: Cross-layer comparison, paper-style visualization       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1ï¸âƒ£ `visualize_attention()` - Detailed Heatmaps
**Má»¥c Ä‘Ã­ch:** Hiá»ƒn thá»‹ FULL attention matrix (2D heatmap) cho tá»«ng (layer, head) cá»¥ thá»ƒ

**Input:**
- `layers_heads`: List of (layer_idx, head_idx) - VD: [(0, 0), (14, 0), (27, 27)]
- Má»—i subplot hiá»ƒn thá»‹ 1 head cá»¥ thá»ƒ

**Output:** 
- Grid cá»§a heatmaps 2D
- Má»—i cell = attention score tá»« query token i â†’ key token j
- CÃ³ token labels trÃªn axes

**Use case:**
- PhÃ¢n tÃ­ch chi tiáº¿t attention matrix
- So sÃ¡nh specific heads
- Debug attention patterns

---

### 2ï¸âƒ£ `visualize_attention_patterns()` - 1D Pattern Analysis
**Má»¥c Ä‘Ã­ch:** Summarize attention thÃ nh 1D pattern (bar chart)

**Input:**
- `layers_heads`: List of (layer_idx, head_idx) 
- `pattern_type`: 
  - 'mean' â†’ Average attention nháº­n Ä‘Æ°á»£c bá»Ÿi má»—i position
  - 'max' â†’ Max attention nháº­n Ä‘Æ°á»£c
  - 'specific_token' â†’ Attention FROM má»™t token cá»¥ thá»ƒ

**Output:**
- Bar charts (1D)
- Má»—i bar = aggregated attention score cho 1 position

**Use case:**
- TÃ¬m tokens nÃ o nháº­n nhiá»u attention (important tokens)
- PhÃ¢n tÃ­ch attention FROM má»™t token quan trá»ng
- So sÃ¡nh attention distribution across layers

---

### 3ï¸âƒ£ `visualize_attention_layers()` - Multi-Layer Overview (Paper Style)
**Má»¥c Ä‘Ã­ch:** Hiá»ƒn thá»‹ attention patterns ACROSS nhiá»u layers (giá»‘ng Figure 2 trong paper)

**Input:**
- `layer_indices`: List of layers - VD: [0, 1, 2, 9, 16, 23, 27]
- `head_idx`: 
  - None â†’ Average across ALL heads (recommended)
  - 0, 1, 2... â†’ Specific head

**Output:**
- Grid cá»§a heatmaps
- Má»—i subplot = 1 layer
- Consistent colorbar (Ä‘á»ƒ so sÃ¡nh giá»¯a layers)
- Colormap RdBu_r (red=high, blue=low)

**Use case:**
- PhÃ¢n tÃ­ch evolution cá»§a attention patterns qua layers
- TÃ¬m "attention sink" phenomenon
- Paper-style visualization
- So sÃ¡nh local vs global attention

---

### ğŸ“Š Quick Comparison Table

| Feature | visualize_attention | visualize_attention_patterns | visualize_attention_layers |
|---------|----------------------|-------------------------------|------------------------------|
| **Plot Type** | 2D Heatmap | 1D Bar Chart | 2D Heatmap Grid |
| **Input** | (layer, head) pairs | (layer, head) pairs | Layer indices only |
| **Head Selection** | Must specify | Must specify | Can average all heads |
| **Output** | Full attention matrix | Aggregated pattern | Multiple layers |
| **Best For** | Detailed analysis | Finding important tokens | Cross-layer comparison |
| **Paper Style** | âŒ | âŒ | âœ… |

---

### ğŸ’¡ Workflow Recommendation

**Step 1:** DÃ¹ng `visualize_attention_layers()` Ä‘á»ƒ cÃ³ overview
â†’ TÃ¬m layers thÃº vá»‹ (local pattern, attention sink, etc.)

**Step 2:** DÃ¹ng `visualize_attention()` Ä‘á»ƒ phÃ¢n tÃ­ch chi tiáº¿t layers/heads cá»¥ thá»ƒ
â†’ Xem full attention matrix

**Step 3:** DÃ¹ng `visualize_attention_patterns()` Ä‘á»ƒ tÃ¬m important tokens
â†’ Tokens nÃ o Ä‘Æ°á»£c attend nhiá»u? Token X attend vÃ o Ä‘Ã¢u?


### ğŸ¯ What to Look For in High Entropy Attention Patterns

**Yellow cross-hairs** = Position cá»§a high entropy token

**Patterns thÃº vá»‹:**

1. **Broad attention distribution** (nhiá»u mÃ u Ä‘á» scattered)
   â†’ Model Ä‘ang "thinking", xem xÃ©t nhiá»u context

2. **Narrow attention** (Ã­t mÃ u Ä‘á», concentrated)
   â†’ Model dá»±a vÃ o specific tokens Ä‘á»ƒ decide

3. **Attention to earlier tokens** (mÃ u Ä‘á» á»Ÿ bÃªn trÃ¡i)
   â†’ Model "looking back" Ä‘á»ƒ láº¥y information

4. **Self-attention spike** (mÃ u Ä‘á» á»Ÿ yellow cross)
   â†’ Token attend strongly to itself

5. **Different patterns across layers:**
   - Early layers: Usually local attention
   - Middle layers: May show reasoning patterns
   - Late layers: Final decision making

**Use cases:**
- Debug model reasoning: Táº¡i sao model uncertain á»Ÿ token nÃ y?
- Find decision points: Token nÃ o lÃ  "turning points" trong reasoning?
- Understand errors: Khi model sai, attention patterns nhÆ° tháº¿ nÃ o?
