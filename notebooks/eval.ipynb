{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fc09e6",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set up the evaluation parameters (from evaluate.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b46d883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Step-JEPA Evaluation Configuration\n",
      "================================================================================\n",
      "Model Path: ./checkpoints_adapted\n",
      "Test File: ../datasets/gsm8k_test.jsonl\n",
      "Output File: ./evaluation_results.jsonl\n",
      "Max New Tokens: 1536\n",
      "Temperature: 0.0\n",
      "Max Examples: None\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters from evaluate.sh\n",
    "MODEL_PATH = \"./checkpoints_adapted\"\n",
    "TEST_FILE = \"../datasets/gsm8k_test.jsonl\"\n",
    "OUTPUT_FILE = \"./evaluation_results.jsonl\"\n",
    "MAX_NEW_TOKENS = 1536\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "MAX_EXAMPLES = None  # Limit for testing\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Step-JEPA Evaluation Configuration\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model Path: {MODEL_PATH}\")\n",
    "print(f\"Test File: {TEST_FILE}\")\n",
    "print(f\"Output File: {OUTPUT_FILE}\")\n",
    "print(f\"Max New Tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"Temperature: {TEMPERATURE}\")\n",
    "print(f\"Max Examples: {MAX_EXAMPLES}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc83c02",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e06291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modaluser/base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc5def",
   "metadata": {},
   "source": [
    "## 3. Helper Functions for Answer Extraction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9107b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined:\n",
      "  - extract_boxed_answer()\n",
      "  - extract_hash_answer()\n",
      "  - extract_final_number()\n",
      "  - normalize_answer()\n",
      "  - extract_answer_from_generated()\n",
      "  - eval_gsm8k()\n"
     ]
    }
   ],
   "source": [
    "def extract_boxed_answer(text):\n",
    "    \"\"\"Extract answer from \\\\boxed{} format (used in DeepSeek system prompt)\"\"\"\n",
    "    pattern = r'\\\\boxed\\{([^}]+)\\}'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "        return normalize_answer(answer)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_hash_answer(text):\n",
    "    \"\"\"Extract answer from #### format (GSM8K standard format)\"\"\"\n",
    "    pattern = r'\\n#### (.+)$'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "        return normalize_answer(answer)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_final_number(text):\n",
    "    \"\"\"Try to extract the last number from the text as a fallback\"\"\"\n",
    "    numbers = re.findall(r'[-+]?(?:\\d*\\.*\\d+)', text)\n",
    "    if numbers:\n",
    "        return normalize_answer(numbers[-1])\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_answer(answer):\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    if answer is None:\n",
    "        return None\n",
    "    \n",
    "    # Remove common text patterns\n",
    "    answer = answer.replace('$', '').replace(',', '').strip()\n",
    "    \n",
    "    # Try to convert to number and normalize\n",
    "    try:\n",
    "        num = float(answer)\n",
    "        # If it's a whole number, return as int\n",
    "        if num.is_integer():\n",
    "            return str(int(num))\n",
    "        else:\n",
    "            # Round to reasonable precision\n",
    "            return f\"{num:.10g}\"\n",
    "    except (ValueError, TypeError):\n",
    "        # If not a number, return cleaned string\n",
    "        return answer.strip()\n",
    "\n",
    "\n",
    "def extract_answer_from_generated(generated_text):\n",
    "    \"\"\"Extract answer from generated text - try multiple formats\"\"\"\n",
    "    # Try boxed format first (from DeepSeek system prompt)\n",
    "    answer = extract_boxed_answer(generated_text)\n",
    "    if answer is not None:\n",
    "        return answer\n",
    "    \n",
    "    # Try #### format (GSM8K standard)\n",
    "    answer = extract_hash_answer(generated_text)\n",
    "    if answer is not None:\n",
    "        return answer\n",
    "    \n",
    "    # Fallback: try to extract last number\n",
    "    answer = extract_final_number(generated_text)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def eval_gsm8k(generated, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluate GSM8K answer.\n",
    "    \n",
    "    Args:\n",
    "        generated: Generated response text\n",
    "        ground_truth: Ground truth in GSM8K format (with ####)\n",
    "    \n",
    "    Returns:\n",
    "        (is_correct, gt_answer, gen_answer)\n",
    "    \"\"\"\n",
    "    # Extract ground truth answer\n",
    "    gt_answer = extract_hash_answer(ground_truth)\n",
    "    \n",
    "    # Extract generated answer\n",
    "    gen_answer = extract_answer_from_generated(generated)\n",
    "    \n",
    "    # Compare\n",
    "    is_correct = (gt_answer is not None and \n",
    "                  gen_answer is not None and \n",
    "                  gt_answer == gen_answer)\n",
    "    \n",
    "    return is_correct, gt_answer, gen_answer\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined:\")\n",
    "print(\"  - extract_boxed_answer()\")\n",
    "print(\"  - extract_hash_answer()\")\n",
    "print(\"  - extract_final_number()\")\n",
    "print(\"  - normalize_answer()\")\n",
    "print(\"  - extract_answer_from_generated()\")\n",
    "print(\"  - eval_gsm8k()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d867e",
   "metadata": {},
   "source": [
    "## 4. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb789d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: ../datasets/gsm8k_test.jsonl\n",
      "✓ Loaded 1319 test examples\n",
      "\n",
      "Total examples to evaluate: 1319\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading test data from: {TEST_FILE}\")\n",
    "\n",
    "test_data = []\n",
    "with open(TEST_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line.strip())\n",
    "        test_data.append(example)\n",
    "\n",
    "print(f\"✓ Loaded {len(test_data)} test examples\")\n",
    "\n",
    "# Limit to MAX_EXAMPLES\n",
    "if MAX_EXAMPLES is not None:\n",
    "    test_data = test_data[:MAX_EXAMPLES]\n",
    "    print(f\"✓ Limited to first {MAX_EXAMPLES} examples for testing\")\n",
    "\n",
    "print(f\"\\nTotal examples to evaluate: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ceed16",
   "metadata": {},
   "source": [
    "## 5. Load Model with vLLM (Fast Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eed47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with vLLM from: checkpoints_adapted\n",
      "================================================================================\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'checkpoints_adapted' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with vLLM for optimized inference...\n",
      "INFO 01-03 04:20:48 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'disable_log_stats': True, 'model': 'checkpoints_adapted'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 04:20:48 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
      "INFO 01-03 04:20:48 [model.py:1661] Using max model len 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 04:20:48,632\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 04:20:48 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'checkpoints_adapted' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:49 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='checkpoints_adapted', speculative_config=None, tokenizer='checkpoints_adapted', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=checkpoints_adapted, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:50 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.7.80:20101 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:50 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:51 [gpu_model_runner.py:3562] Starting to load model checkpoints_adapted...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:51 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:52 [default_loader.py:308] Loading weights took 0.53 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:53 [gpu_model_runner.py:3659] Model loading took 2.3185 GiB memory and 0.966589 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:58 [backends.py:643] Using cache directory: /home/modaluser/.cache/vllm/torch_compile_cache/8a0a5f3b72/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:20:58 [backends.py:703] Dynamo bytecode transform time: 4.90 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:21:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.616 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:21:00 [monitor.py:34] torch.compile takes 5.52 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:21:01 [gpu_worker.py:375] Available KV cache memory: 67.77 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:21:01 [kv_cache_utils.py:1291] GPU KV cache size: 2,220,592 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:21:01 [kv_cache_utils.py:1296] Maximum concurrency for 4,096 tokens per request: 542.14x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 37.23it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 46.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:21:04 [gpu_model_runner.py:4587] Graph capturing finished in 3 secs, took 0.31 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=29803)\u001b[0;0m INFO 01-03 04:21:04 [core.py:259] init engine (profile, create kv cache, warmup model) took 11.00 seconds\n",
      "INFO 01-03 04:21:04 [llm.py:360] Supported tasks: ['generate']\n",
      "\n",
      "✓ Model loaded successfully with vLLM\n",
      "  vLLM provides highly optimized inference with PagedAttention\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_path = Path(MODEL_PATH)\n",
    "\n",
    "print(f\"Loading model with vLLM from: {model_path}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model with vLLM for fast inference\n",
    "print(\"Loading model with vLLM for optimized inference...\")\n",
    "llm = LLM(\n",
    "    model=str(model_path),\n",
    "    # dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=1,  # Adjust based on number of GPUs\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,  # Adjust based on your needs\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully with vLLM\")\n",
    "print(\"  vLLM provides highly optimized inference with PagedAttention\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ecf3c",
   "metadata": {},
   "source": [
    "## 6. Define Generation Function (vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6437afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ generate_response() function defined (vLLM-based)\n"
     ]
    }
   ],
   "source": [
    "def generate_response(llm, tokenizer, messages, max_new_tokens=1536, temperature=0.0):\n",
    "    \"\"\"Generate response for a given prompt using vLLM\"\"\"\n",
    "    # Format the conversation\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Set up sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature if temperature > 0 else 0.0,\n",
    "        max_tokens=max_new_tokens,\n",
    "        top_p=1.0 if temperature == 0 else 0.95,\n",
    "    )\n",
    "    \n",
    "    # Generate with vLLM\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    \n",
    "    # Extract generated text\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "print(\"✓ generate_response() function defined (vLLM-based)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70669dc0",
   "metadata": {},
   "source": [
    "## 7. Test Generation on First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0e3df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation on first example...\n",
      "================================================================================\n",
      "QUESTION:\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 102.81it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it, est. speed input: 68.13 toks/s, output: 175.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED RESPONSE:\n",
      "To determine how much Janet makes at the farmers' market, we need to calculate the total number of eggs she has and then subtract the eggs she eats and uses.\n",
      "\n",
      "1. **Total number of eggs per day:**\n",
      "   - Janet's ducks lay 16 eggs per day.\n",
      "   - She eats 3 eggs for breakfast.\n",
      "   - She bakes muffins with 4 eggs each day.\n",
      "   - Total eggs per day = 16 (ducks) + 3 (breakfast) + 4 (muffins) = 23 eggs.\n",
      "\n",
      "2. **Eggs used or sold:**\n",
      "   - Janet eats 3 eggs for breakfast.\n",
      "   - She bakes muffins with 4 eggs each day.\n",
      "   - Total eggs used or sold per day = 3 (breakfast) + 4 (muffins) = 7 eggs.\n",
      "\n",
      "3. **Eggs left for sale:**\n",
      "   - Total eggs per day = 23.\n",
      "   - Eggs used or sold = 7.\n",
      "   - Eggs left for sale = 23 - 7 = 16 eggs.\n",
      "\n",
      "4. **Daily earnings from eggs sold:**\n",
      "   - Janet sells each egg for $2.\n",
      "   - Daily earnings = 16 eggs × $2 = $32.\n",
      "\n",
      "**Conclusion:**\n",
      "Janet makes $32 every day at the farmers' market.\n",
      "\n",
      "================================================================================\n",
      "GROUND TRUTH:\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on first example\n",
    "test_example = test_data[0]\n",
    "test_messages = test_example[\"messages\"]\n",
    "ground_truth = test_messages[-1][\"content\"]\n",
    "input_messages = test_messages[:-1]  # Exclude assistant's answer\n",
    "\n",
    "print(\"Testing generation on first example...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"QUESTION:\")\n",
    "print(input_messages[1][\"content\"])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Generate response\n",
    "generated_response = generate_response(\n",
    "    llm, tokenizer, input_messages,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    temperature=TEMPERATURE\n",
    ")\n",
    "\n",
    "print(\"GENERATED RESPONSE:\")\n",
    "print(generated_response)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GROUND TRUTH:\")\n",
    "print(ground_truth)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df8993",
   "metadata": {},
   "source": [
    "### Evaluate Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bcae01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION RESULT:\n",
      "================================================================================\n",
      "Ground Truth Answer: 18\n",
      "Generated Answer: 32\n",
      "Correct: ✗ NO\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test example\n",
    "is_correct, gt_answer, gen_answer = eval_gsm8k(generated_response, ground_truth)\n",
    "\n",
    "print(\"EVALUATION RESULT:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Ground Truth Answer: {gt_answer}\")\n",
    "print(f\"Generated Answer: {gen_answer}\")\n",
    "print(f\"Correct: {'✓ YES' if is_correct else '✗ NO'}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175769fe",
   "metadata": {},
   "source": [
    "## 8. Run Full Evaluation with Batch Inference\n",
    "\n",
    "Single pass: prepare prompts → batch generate → evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db591a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 1319 prompts for batch inference...\n",
      "================================================================================\n",
      "✓ Prepared 1319 prompts\n",
      "First prompt preview (first 200 chars):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 03 Jan 2026\n",
      "\n",
      "Please solve the problem step by step (separate steps with double newlines),...\n",
      "================================================================================\n",
      "\n",
      "Generating responses with vLLM batch inference...\n",
      "This is much faster than sequential generation!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:  52%|█████▏    | 682/1319 [00:00<00:00, 2285.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 2277.37it/s]\n",
      "Processed prompts: 100%|██████████| 1319/1319 [00:45<00:00, 29.26it/s, est. speed input: 3920.13 toks/s, output: 10125.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 1319 responses\n",
      "================================================================================\n",
      "\n",
      "Evaluating results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1319/1319 [00:00<00:00, 35664.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 examples: 4/10 correct (40.00%)\n",
      "After 20 examples: 5/20 correct (25.00%)\n",
      "After 30 examples: 9/30 correct (30.00%)\n",
      "After 40 examples: 14/40 correct (35.00%)\n",
      "After 50 examples: 19/50 correct (38.00%)\n",
      "After 60 examples: 22/60 correct (36.67%)\n",
      "After 70 examples: 24/70 correct (34.29%)\n",
      "After 80 examples: 30/80 correct (37.50%)\n",
      "After 90 examples: 36/90 correct (40.00%)\n",
      "After 100 examples: 39/100 correct (39.00%)\n",
      "After 110 examples: 43/110 correct (39.09%)\n",
      "After 120 examples: 47/120 correct (39.17%)\n",
      "After 130 examples: 51/130 correct (39.23%)\n",
      "After 140 examples: 59/140 correct (42.14%)\n",
      "After 150 examples: 62/150 correct (41.33%)\n",
      "After 160 examples: 66/160 correct (41.25%)\n",
      "After 170 examples: 73/170 correct (42.94%)\n",
      "After 180 examples: 77/180 correct (42.78%)\n",
      "After 190 examples: 79/190 correct (41.58%)\n",
      "After 200 examples: 85/200 correct (42.50%)\n",
      "After 210 examples: 89/210 correct (42.38%)\n",
      "After 220 examples: 96/220 correct (43.64%)\n",
      "After 230 examples: 103/230 correct (44.78%)\n",
      "After 240 examples: 111/240 correct (46.25%)\n",
      "After 250 examples: 115/250 correct (46.00%)\n",
      "After 260 examples: 120/260 correct (46.15%)\n",
      "After 270 examples: 126/270 correct (46.67%)\n",
      "After 280 examples: 132/280 correct (47.14%)\n",
      "After 290 examples: 139/290 correct (47.93%)\n",
      "After 300 examples: 142/300 correct (47.33%)\n",
      "After 310 examples: 146/310 correct (47.10%)\n",
      "After 320 examples: 153/320 correct (47.81%)\n",
      "After 330 examples: 156/330 correct (47.27%)\n",
      "After 340 examples: 162/340 correct (47.65%)\n",
      "After 350 examples: 167/350 correct (47.71%)\n",
      "After 360 examples: 172/360 correct (47.78%)\n",
      "After 370 examples: 176/370 correct (47.57%)\n",
      "After 380 examples: 181/380 correct (47.63%)\n",
      "After 390 examples: 186/390 correct (47.69%)\n",
      "After 400 examples: 192/400 correct (48.00%)\n",
      "After 410 examples: 195/410 correct (47.56%)\n",
      "After 420 examples: 200/420 correct (47.62%)\n",
      "After 430 examples: 204/430 correct (47.44%)\n",
      "After 440 examples: 212/440 correct (48.18%)\n",
      "After 450 examples: 218/450 correct (48.44%)\n",
      "After 460 examples: 223/460 correct (48.48%)\n",
      "After 470 examples: 230/470 correct (48.94%)\n",
      "After 480 examples: 239/480 correct (49.79%)\n",
      "After 490 examples: 248/490 correct (50.61%)\n",
      "After 500 examples: 253/500 correct (50.60%)\n",
      "After 510 examples: 256/510 correct (50.20%)\n",
      "After 520 examples: 263/520 correct (50.58%)\n",
      "After 530 examples: 265/530 correct (50.00%)\n",
      "After 540 examples: 269/540 correct (49.81%)\n",
      "After 550 examples: 274/550 correct (49.82%)\n",
      "After 560 examples: 283/560 correct (50.54%)\n",
      "After 570 examples: 286/570 correct (50.18%)\n",
      "After 580 examples: 289/580 correct (49.83%)\n",
      "After 590 examples: 292/590 correct (49.49%)\n",
      "After 600 examples: 297/600 correct (49.50%)\n",
      "After 610 examples: 301/610 correct (49.34%)\n",
      "After 620 examples: 307/620 correct (49.52%)\n",
      "After 630 examples: 312/630 correct (49.52%)\n",
      "After 640 examples: 319/640 correct (49.84%)\n",
      "After 650 examples: 322/650 correct (49.54%)\n",
      "After 660 examples: 326/660 correct (49.39%)\n",
      "After 670 examples: 332/670 correct (49.55%)\n",
      "After 680 examples: 339/680 correct (49.85%)\n",
      "After 690 examples: 344/690 correct (49.86%)\n",
      "After 700 examples: 349/700 correct (49.86%)\n",
      "After 710 examples: 355/710 correct (50.00%)\n",
      "After 720 examples: 363/720 correct (50.42%)\n",
      "After 730 examples: 369/730 correct (50.55%)\n",
      "After 740 examples: 374/740 correct (50.54%)\n",
      "After 750 examples: 380/750 correct (50.67%)\n",
      "After 760 examples: 386/760 correct (50.79%)\n",
      "After 770 examples: 392/770 correct (50.91%)\n",
      "After 780 examples: 397/780 correct (50.90%)\n",
      "After 790 examples: 400/790 correct (50.63%)\n",
      "After 800 examples: 406/800 correct (50.75%)\n",
      "After 810 examples: 409/810 correct (50.49%)\n",
      "After 820 examples: 413/820 correct (50.37%)\n",
      "After 830 examples: 419/830 correct (50.48%)\n",
      "After 840 examples: 425/840 correct (50.60%)\n",
      "After 850 examples: 431/850 correct (50.71%)\n",
      "After 860 examples: 438/860 correct (50.93%)\n",
      "After 870 examples: 445/870 correct (51.15%)\n",
      "After 880 examples: 450/880 correct (51.14%)\n",
      "After 890 examples: 456/890 correct (51.24%)\n",
      "After 900 examples: 461/900 correct (51.22%)\n",
      "After 910 examples: 467/910 correct (51.32%)\n",
      "After 920 examples: 475/920 correct (51.63%)\n",
      "After 930 examples: 477/930 correct (51.29%)\n",
      "After 940 examples: 482/940 correct (51.28%)\n",
      "After 950 examples: 489/950 correct (51.47%)\n",
      "After 960 examples: 493/960 correct (51.35%)\n",
      "After 970 examples: 496/970 correct (51.13%)\n",
      "After 980 examples: 502/980 correct (51.22%)\n",
      "After 990 examples: 510/990 correct (51.52%)\n",
      "After 1000 examples: 512/1000 correct (51.20%)\n",
      "After 1010 examples: 515/1010 correct (50.99%)\n",
      "After 1020 examples: 516/1020 correct (50.59%)\n",
      "After 1030 examples: 519/1030 correct (50.39%)\n",
      "After 1040 examples: 524/1040 correct (50.38%)\n",
      "After 1050 examples: 525/1050 correct (50.00%)\n",
      "After 1060 examples: 532/1060 correct (50.19%)\n",
      "After 1070 examples: 536/1070 correct (50.09%)\n",
      "After 1080 examples: 539/1080 correct (49.91%)\n",
      "After 1090 examples: 543/1090 correct (49.82%)\n",
      "After 1100 examples: 549/1100 correct (49.91%)\n",
      "After 1110 examples: 556/1110 correct (50.09%)\n",
      "After 1120 examples: 559/1120 correct (49.91%)\n",
      "After 1130 examples: 563/1130 correct (49.82%)\n",
      "After 1140 examples: 570/1140 correct (50.00%)\n",
      "After 1150 examples: 576/1150 correct (50.09%)\n",
      "After 1160 examples: 582/1160 correct (50.17%)\n",
      "After 1170 examples: 587/1170 correct (50.17%)\n",
      "After 1180 examples: 592/1180 correct (50.17%)\n",
      "After 1190 examples: 596/1190 correct (50.08%)\n",
      "After 1200 examples: 598/1200 correct (49.83%)\n",
      "After 1210 examples: 604/1210 correct (49.92%)\n",
      "After 1220 examples: 609/1220 correct (49.92%)\n",
      "After 1230 examples: 616/1230 correct (50.08%)\n",
      "After 1240 examples: 620/1240 correct (50.00%)\n",
      "After 1250 examples: 623/1250 correct (49.84%)\n",
      "After 1260 examples: 630/1260 correct (50.00%)\n",
      "After 1270 examples: 634/1270 correct (49.92%)\n",
      "After 1280 examples: 638/1280 correct (49.84%)\n",
      "After 1290 examples: 642/1290 correct (49.77%)\n",
      "After 1300 examples: 646/1300 correct (49.69%)\n",
      "After 1310 examples: 651/1310 correct (49.69%)\n",
      "\n",
      "================================================================================\n",
      "✓ Batch evaluation complete!\n",
      "Results saved to: ./evaluation_results.jsonl\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running batch evaluation on {len(test_data)} examples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare all prompts\n",
    "all_prompts = []\n",
    "ground_truths = []\n",
    "questions = []\n",
    "\n",
    "for example in test_data:\n",
    "    messages = example[\"messages\"]\n",
    "    ground_truth = messages[-1][\"content\"]\n",
    "    input_messages = messages[:-1]  # Exclude assistant's answer\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        input_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    all_prompts.append(prompt)\n",
    "    ground_truths.append(ground_truth)\n",
    "    questions.append(messages[1][\"content\"])\n",
    "\n",
    "# Batch generate with vLLM (single call for all prompts)\n",
    "print(f\"Batch generating {len(all_prompts)} responses with vLLM...\")\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE if TEMPERATURE > 0 else 0.0,\n",
    "    max_tokens=MAX_NEW_TOKENS,\n",
    "    top_p=1.0 if TEMPERATURE == 0 else 0.95,\n",
    ")\n",
    "outputs = llm.generate(all_prompts, sampling_params)\n",
    "print(f\"✓ Generation complete\\n\")\n",
    "\n",
    "# Evaluate and save results\n",
    "results = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    for idx, (output, ground_truth, question) in enumerate(tqdm(\n",
    "        zip(outputs, ground_truths, questions), \n",
    "        total=len(outputs),\n",
    "        desc=\"Evaluating\"\n",
    "    )):\n",
    "        try:\n",
    "            # Extract generated text\n",
    "            generated_response = output.outputs[0].text.strip()\n",
    "            \n",
    "            # Evaluate\n",
    "            is_correct, gt_answer, gen_answer = eval_gsm8k(generated_response, ground_truth)\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "            \n",
    "            # Compute accuracy so far\n",
    "            accuracy = correct_count / total_count * 100\n",
    "            \n",
    "            # Create result entry\n",
    "            result = {\n",
    "                \"index\": idx,\n",
    "                \"question\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"generated_response\": generated_response,\n",
    "                \"gt_answer\": gt_answer,\n",
    "                \"gen_answer\": gen_answer,\n",
    "                \"correct\": is_correct,\n",
    "                \"accuracy_so_far\": accuracy\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Write to file\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "            f.flush()\n",
    "            \n",
    "            # Print progress every 10 examples\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"After {idx + 1} examples: {correct_count}/{total_count} correct ({accuracy:.2f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error at index {idx}: {e}\")\n",
    "            result = {\n",
    "                \"index\": idx,\n",
    "                \"question\": question,\n",
    "                \"error\": str(e),\n",
    "                \"correct\": False\n",
    "            }\n",
    "            results.append(result)\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "            f.flush()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Batch evaluation complete!\")\n",
    "print(f\"Results saved to: {OUTPUT_FILE}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b6e1f",
   "metadata": {},
   "source": [
    "## 9. Display Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total examples: {total_count}\")\n",
    "print(f\"Correct: {correct_count}\")\n",
    "print(f\"Incorrect: {total_count - correct_count}\")\n",
    "print(f\"Accuracy: {correct_count / total_count * 100:.2f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    \"model_path\": MODEL_PATH,\n",
    "    \"test_file\": TEST_FILE,\n",
    "    \"total_examples\": total_count,\n",
    "    \"correct\": correct_count,\n",
    "    \"accuracy\": correct_count / total_count if total_count > 0 else 0,\n",
    "    \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "    \"temperature\": TEMPERATURE,\n",
    "}\n",
    "\n",
    "summary_file = OUTPUT_FILE.replace('.jsonl', '_summary.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915c08b",
   "metadata": {},
   "source": [
    "## 10. Analyze Results\n",
    "\n",
    "Let's look at some correct and incorrect examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate correct and incorrect results\n",
    "correct_results = [r for r in results if r.get(\"correct\", False)]\n",
    "incorrect_results = [r for r in results if not r.get(\"correct\", False)]\n",
    "\n",
    "print(f\"Correct examples: {len(correct_results)}\")\n",
    "print(f\"Incorrect examples: {len(incorrect_results)}\")\n",
    "\n",
    "# Show first few correct examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE CORRECT EXAMPLES:\")\n",
    "print(\"=\" * 80)\n",
    "for i, result in enumerate(correct_results[:3]):\n",
    "    print(f\"\\n--- Correct Example {i+1} ---\")\n",
    "    print(f\"Question: {result['question'][:100]}...\")\n",
    "    print(f\"GT Answer: {result['gt_answer']}\")\n",
    "    print(f\"Gen Answer: {result['gen_answer']}\")\n",
    "\n",
    "# Show first few incorrect examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE INCORRECT EXAMPLES:\")\n",
    "print(\"=\" * 80)\n",
    "for i, result in enumerate(incorrect_results[:3]):\n",
    "    print(f\"\\n--- Incorrect Example {i+1} ---\")\n",
    "    if 'question' in result:\n",
    "        print(f\"Question: {result['question'][:100]}...\")\n",
    "    if 'gt_answer' in result:\n",
    "        print(f\"GT Answer: {result['gt_answer']}\")\n",
    "    if 'gen_answer' in result:\n",
    "        print(f\"Gen Answer: {result['gen_answer']}\")\n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f567d2c",
   "metadata": {},
   "source": [
    "### Detailed Inspection of One Incorrect Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca440a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one incorrect example for detailed inspection\n",
    "if incorrect_results:\n",
    "    example = incorrect_results[0]\n",
    "    \n",
    "    print(\"DETAILED INCORRECT EXAMPLE:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nQUESTION:\")\n",
    "    print(example.get('question', 'N/A'))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GROUND TRUTH FULL RESPONSE:\")\n",
    "    print(example.get('ground_truth', 'N/A'))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERATED RESPONSE:\")\n",
    "    print(example.get('generated_response', 'N/A'))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXTRACTED ANSWERS:\")\n",
    "    print(f\"  Ground Truth: {example.get('gt_answer', 'N/A')}\")\n",
    "    print(f\"  Generated: {example.get('gen_answer', 'N/A')}\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No incorrect examples found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c45df7",
   "metadata": {},
   "source": [
    "## 11. Accuracy Over Time\n",
    "\n",
    "Plot how accuracy evolved during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe54585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract accuracy over time\n",
    "accuracies = [r.get('accuracy_so_far', 0) for r in results if 'accuracy_so_far' in r]\n",
    "indices = list(range(1, len(accuracies) + 1))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(indices, accuracies, linewidth=2)\n",
    "plt.axhline(y=accuracies[-1] if accuracies else 0, color='r', linestyle='--', \n",
    "            label=f'Final Accuracy: {accuracies[-1]:.2f}%' if accuracies else 'N/A')\n",
    "plt.xlabel('Number of Examples Evaluated')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Over Evaluation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final accuracy: {accuracies[-1]:.2f}%\" if accuracies else \"No data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
