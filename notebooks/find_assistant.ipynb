{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e776a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# login to huggingface\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52877947",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../datasets/gsm8k_step_jepa.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files=data_file, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc1b1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indices = []\n",
    "for i, sample in enumerate(dataset):\n",
    "    for msg in sample.get(\"messages\", []):\n",
    "        if msg.get(\"role\") == \"user\" and \"\\n\\n\" in (msg.get(\"content\") or \"\"):\n",
    "            bad_indices.append(i)\n",
    "            print(msg.get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c911d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MAX_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41141ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bed93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80f3abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_labels(messages, tokenizer, input_ids, attention_mask):\n",
    "        \"\"\"Create labels with input tokens masked (-100)\"\"\"\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # Mask padding tokens in labels\n",
    "        for i, mask in enumerate(attention_mask):\n",
    "            if mask == 0:  # Padding token\n",
    "                labels[i] = -100\n",
    "        \n",
    "        # Find assistant responses and unmask only those tokens\n",
    "        for msg in messages:\n",
    "            if msg['role'] == 'assistant':\n",
    "                assistant_content = msg['content']\n",
    "                \n",
    "                # Find where this assistant response appears in the tokenized text\n",
    "                # assistant_tokens = tokenizer.encode(assistant_content, add_special_tokens=False)\n",
    "                assistant_with_eot = assistant_content + tokenizer.eos_token\n",
    "                assistant_tokens = tokenizer.encode(assistant_with_eot, add_special_tokens=False)\n",
    "\n",
    "                \n",
    "                # Find the position of assistant response in input_ids\n",
    "                decoded_assistant = [tokenizer.decode(item) for item in assistant_tokens]\n",
    "                decoded_input = [tokenizer.decode(item) for item in input_ids]\n",
    "\n",
    "                print(f\"decoded_input: {decoded_input}\")\n",
    "                print(f\"decoded_assistant: {decoded_assistant}\")\n",
    "                for i in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "                    # Only check non-padding tokens\n",
    "                    if debug == 4:\n",
    "                        print(f\"=======input_ids: {input_ids[i:i+len(assistant_tokens)]}\")\n",
    "                        print(f\"assistant_tokens: {assistant_tokens}\")\n",
    "                    # if attention_mask[i] == 1 and input_ids[i:i+len(assistant_tokens)] == assistant_tokens:\n",
    "                    if attention_mask[i] == 1 and decoded_input[i:i+len(assistant_tokens)] == decoded_assistant:\n",
    "                        # Unmask the assistant response tokens\n",
    "                        for j in range(i, min(i + len(assistant_tokens), len(input_ids))):\n",
    "                            if attention_mask[j] == 1:  # Only unmask non-padding tokens\n",
    "                                labels[j] = input_ids[j]\n",
    "                        break\n",
    "                \n",
    "                if debug == 4:\n",
    "                    exit(0)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1545279",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Please solve the problem step by step (separate steps with double newlines), \"\n",
    "                       \"but keep it short and put your final answer (do not include any other text or units) \"\n",
    "                       \"within \\\\boxed{}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \"\n",
    "                       \"How many clips did Natalia sell altogether in April and May?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Step 1: April = 48\\n\\nStep 2: May = 48/2 = 24\\n\\nStep 3: Total = 48 + 24 = \\\\boxed{72}\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c97cae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 05 Jan 2026\n",
      "\n",
      "Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Step 1: April = 48\n",
      "\n",
      "Step 2: May = 48/2 = 24\n",
      "\n",
      "Step 3: Total = 48 + 24 = \\boxed{72}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "formatted_chat = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False,\n",
    "                )\n",
    "print(formatted_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c491ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128006,   9125,  ..., 128256, 128256, 128256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer(formatted_chat, return_tensors=\"pt\", \n",
    "                        padding=\"max_length\", \n",
    "                        max_length=MAX_LENGTH,\n",
    "                        add_special_tokens=False,\n",
    "                        )\n",
    "print(tokenized_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a1f728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized_chat['input_ids'][0]\n",
    "attention_mask = tokenized_chat['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1395cb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_input: ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' ', '202', '3', '\\n', 'Today', ' Date', ':', ' ', '05', ' Jan', ' ', '202', '6', '\\n\\n', 'Please', ' solve', ' the', ' problem', ' step', ' by', ' step', ' (', 'se', 'parate', ' steps', ' with', ' double', ' new', 'lines', '),', ' but', ' keep', ' it', ' short', ' and', ' put', ' your', ' final', ' answer', ' (', 'do', ' not', ' include', ' any', ' other', ' text', ' or', ' units', ')', ' within', ' \\\\', 'boxed', '{}.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'N', 'atal', 'ia', ' sold', ' clips', ' to', ' ', '48', ' of', ' her', ' friends', ' in', ' April', ',', ' and', ' then', ' she', ' sold', ' half', ' as', ' many', ' clips', ' in', ' May', '.', ' How', ' many', ' clips', ' did', ' Natal', 'ia', ' sell', ' altogether', ' in', ' April', ' and', ' May', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n', 'Step', ' ', '1', ':', ' April', ' =', ' ', '48', '\\n\\n', 'Step', ' ', '2', ':', ' May', ' =', ' ', '48', '/', '2', ' =', ' ', '24', '\\n\\n', 'Step', ' ', '3', ':', ' Total', ' =', ' ', '48', ' +', ' ', '24', ' =', ' \\\\', 'boxed', '{', '72', '}', '<|eot_id|>', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "decoded_assistant: ['Step', ' ', '1', ':', ' April', ' =', ' ', '48', '\\n\\n', 'Step', ' ', '2', ':', ' May', ' =', ' ', '48', '/', '2', ' =', ' ', '24', '\\n\\n', 'Step', ' ', '3', ':', ' Total', ' =', ' ', '48', ' +', ' ', '24', ' =', ' \\\\', 'boxed', '{', '72', '}', '<|eot_id|>']\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, tensor(8468), tensor(220), tensor(16), tensor(25), tensor(5936), tensor(284), tensor(220), tensor(2166), tensor(271), tensor(8468), tensor(220), tensor(17), tensor(25), tensor(3297), tensor(284), tensor(220), tensor(2166), tensor(14), tensor(17), tensor(284), tensor(220), tensor(1187), tensor(271), tensor(8468), tensor(220), tensor(18), tensor(25), tensor(10884), tensor(284), tensor(220), tensor(2166), tensor(489), tensor(220), tensor(1187), tensor(284), tensor(1144), tensor(80175), tensor(90), tensor(5332), tensor(92), tensor(128009), -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = create_masked_labels(messages, tokenizer, tokenized_chat['input_ids'][0], tokenized_chat['attention_mask'][0])\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cec7c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: April = 48\n",
      "\n",
      "Step 2: May = 48/2 = 24\n",
      "\n",
      "Step 3: Total = 48 + 24 = \\boxed{72}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "labels_ids = [label_id for label_id in labels if label_id != -100]\n",
    "print(tokenizer.decode(labels_ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e78cf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [-100] * len(input_ids)\n",
    "        \n",
    "# # Mask padding tokens in labels\n",
    "# for i, mask in enumerate(attention_mask):\n",
    "#     if mask == 0:  # Padding token\n",
    "#         labels[i] = -100\n",
    "\n",
    "# # Find assistant responses and unmask only those tokens\n",
    "# for msg in messages:\n",
    "#     if msg['role'] == 'assistant':\n",
    "#         assistant_content = msg['content']\n",
    "        \n",
    "#         # Find where this assistant response appears in the tokenized text\n",
    "#         assistant_tokens = tokenizer.encode(assistant_content, add_special_tokens=False)\n",
    "        \n",
    "#         # Find the position of assistant response in input_ids\n",
    "#         decoded_assistant = [tokenizer.decode(item, skip_special_tokens=False) for item in assistant_tokens]\n",
    "#         decoded_input = [tokenizer.decode(item, skip_special_tokens=False) for item in input_ids]\n",
    "\n",
    "#         print(f\"decoded_input: {decoded_input}\")\n",
    "#         print(f\"decoded_assistant: {decoded_assistant}\")\n",
    "#         for i in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "#             # Only check non-padding tokens\n",
    "#             if debug == 4:\n",
    "#                 print(f\"=======input_ids: {input_ids[i:i+len(assistant_tokens)]}\")\n",
    "#                 print(f\"assistant_tokens: {assistant_tokens}\")\n",
    "#             if attention_mask[i] == 1 and decoded_input[i:i+len(assistant_tokens)] == decoded_assistant:\n",
    "#                 # Unmask the assistant response tokens\n",
    "#                 for j in range(i, min(i + len(assistant_tokens), len(input_ids))):\n",
    "#                     if attention_mask[j] == 1:  # Only unmask non-padding tokens\n",
    "#                         labels[j] = input_ids[j]\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f6663f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_input: ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' ', '202', '3', '\\n', 'Today', ' Date', ':', ' ', '05', ' Jan', ' ', '202', '6', '\\n\\n', 'Please', ' solve', ' the', ' problem', ' step', ' by', ' step', ' (', 'se', 'parate', ' steps', ' with', ' double', ' new', 'lines', '),', ' but', ' keep', ' it', ' short', ' and', ' put', ' your', ' final', ' answer', ' (', 'do', ' not', ' include', ' any', ' other', ' text', ' or', ' units', ')', ' within', ' \\\\', 'boxed', '{}.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'N', 'atal', 'ia', ' sold', ' clips', ' to', ' ', '48', ' of', ' her', ' friends', ' in', ' April', ',', ' and', ' then', ' she', ' sold', ' half', ' as', ' many', ' clips', ' in', ' May', '.', ' How', ' many', ' clips', ' did', ' Natal', 'ia', ' sell', ' altogether', ' in', ' April', ' and', ' May', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n', 'Step', ' ', '1', ':', ' April', ' =', ' ', '48', '\\n\\n', 'Step', ' ', '2', ':', ' May', ' =', ' ', '48', '/', '2', ' =', ' ', '24', '\\n\\n', 'Step', ' ', '3', ':', ' Total', ' =', ' ', '48', ' +', ' ', '24', ' =', ' \\\\', 'boxed', '{', '72', '}', '<|eot_id|>']\n",
      "decoded_assistant: ['Step', ' ', '1', ':', ' April', ' =', ' ', '48', '\\n\\n', 'Step', ' ', '2', ':', ' May', ' =', ' ', '48', '/', '2', ' =', ' ', '24', '\\n\\n', 'Step', ' ', '3', ':', ' Total', ' =', ' ', '48', ' +', ' ', '24', ' =', ' \\\\', 'boxed', '{', '72', '}']\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, tensor(8468), tensor(220), tensor(16), tensor(25), tensor(5936), tensor(284), tensor(220), tensor(2166), tensor(271), tensor(8468), tensor(220), tensor(17), tensor(25), tensor(3297), tensor(284), tensor(220), tensor(2166), tensor(14), tensor(17), tensor(284), tensor(220), tensor(1187), tensor(271), tensor(8468), tensor(220), tensor(18), tensor(25), tensor(10884), tensor(284), tensor(220), tensor(2166), tensor(489), tensor(220), tensor(1187), tensor(284), tensor(1144), tensor(80175), tensor(90), tensor(5332), tensor(92), -100]\n"
     ]
    }
   ],
   "source": [
    "labels = create_masked_labels(messages, tokenizer, tokenized_chat['input_ids'][0], tokenized_chat['attention_mask'][0])\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44742523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(8468), tensor(220), tensor(16), tensor(25), tensor(5936), tensor(284), tensor(220), tensor(2166), tensor(271), tensor(8468), tensor(220), tensor(17), tensor(25), tensor(3297), tensor(284), tensor(220), tensor(2166), tensor(14), tensor(17), tensor(284), tensor(220), tensor(1187), tensor(271), tensor(8468), tensor(220), tensor(18), tensor(25), tensor(10884), tensor(284), tensor(220), tensor(2166), tensor(489), tensor(220), tensor(1187), tensor(284), tensor(1144), tensor(80175), tensor(90), tensor(5332), tensor(92)]\n",
      "Step\n",
      " \n",
      "1\n",
      ":\n",
      " April\n",
      " =\n",
      " \n",
      "48\n",
      "\n",
      "\n",
      "\n",
      "Step\n",
      " \n",
      "2\n",
      ":\n",
      " May\n",
      " =\n",
      " \n",
      "48\n",
      "/\n",
      "2\n",
      " =\n",
      " \n",
      "24\n",
      "\n",
      "\n",
      "\n",
      "Step\n",
      " \n",
      "3\n",
      ":\n",
      " Total\n",
      " =\n",
      " \n",
      "48\n",
      " +\n",
      " \n",
      "24\n",
      " =\n",
      " \\\n",
      "boxed\n",
      "{\n",
      "72\n",
      "}\n",
      "================================================\n",
      "Step 1: April = 48\n",
      "\n",
      "Step 2: May = 48/2 = 24\n",
      "\n",
      "Step 3: Total = 48 + 24 = \\boxed{72}\n"
     ]
    }
   ],
   "source": [
    "len(labels)\n",
    "label_ids = [label_id for label_id in labels if label_id != -100]\n",
    "print(label_ids)\n",
    "\n",
    "for tok in label_ids:\n",
    "    print(tokenizer.decode(tok))\n",
    "\n",
    "print(\"================\"*3)\n",
    "\n",
    "print(tokenizer.decode(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f4591b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_input: ['<|begin_of_text|>', '<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' ', '202', '3', '\\n', 'Today', ' Date', ':', ' ', '05', ' Jan', ' ', '202', '6', '\\n\\n', 'Please', ' solve', ' the', ' problem', ' step', ' by', ' step', ' (', 'se', 'parate', ' steps', ' with', ' double', ' new', 'lines', '),', ' but', ' keep', ' it', ' short', ' and', ' put', ' your', ' final', ' answer', ' (', 'do', ' not', ' include', ' any', ' other', ' text', ' or', ' units', ')', ' within', ' \\\\', 'boxed', '{}.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'W', 'eng', ' earns', ' $', '12', ' an', ' hour', ' for', ' babys', 'itting', '.', ' Yesterday', ',', ' she', ' just', ' did', ' ', '50', ' minutes', ' of', ' babys', 'itting', '.', ' How', ' much', ' did', ' she', ' earn', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n', 'First', ',', ' I', ' need', ' to', ' determine', ' how', ' much', ' W', 'eng', ' earned', ' for', ' babys', 'itting', '.', ' She', ' earns', ' $', '12', ' per', ' hour', ',', ' and', ' she', ' worked', ' for', ' ', '50', ' minutes', '.\\n\\n', 'Since', ' her', ' pay', ' rate', ' is', ' given', ' in', ' hours', ',', ' I', ' should', ' convert', ' the', ' ', '50', ' minutes', ' into', ' hours', '.', ' There', ' are', ' ', '60', ' minutes', ' in', ' an', ' hour', ',', ' so', ' ', '50', ' minutes', ' is', ' ', '50', '/', '60', ' hours', ',', ' which', ' simpl', 'ifies', ' to', ' ', '5', '/', '6', ' of', ' an', ' hour', '.\\n\\n', 'Next', ',', ' I', \"'ll\", ' calculate', ' her', ' earnings', ' by', ' multiplying', ' her', ' hourly', ' rate', ' by', ' the', ' number', ' of', ' hours', ' she', ' worked', '.', ' So', ',', ' $', '12', ' multiplied', ' by', ' ', '5', '/', '6', ' equals', ' $', '10', '.\\n\\n', 'Therefore', ',', ' W', 'eng', ' earned', ' $', '10', ' for', ' babys', 'itting', ' yesterday', '.\\n', '</', 'think', '>\\n\\n', '1', '.', ' **', 'D', 'etermine', ' the', ' hourly', ' rate', ' and', ' time', ' worked', ':', '**\\n', '  ', ' -', ' **', 'Hour', 'ly', ' rate', ':**', ' \\\\$', '12', ' per', ' hour', '\\n', '  ', ' -', ' **', 'Time', ' worked', ':**', ' ', '50', ' minutes', '\\n\\n', '2', '.', ' **', 'Convert', ' minutes', ' to', ' hours', ':', '**\\n', '  ', ' \\\\', '[\\n', '  ', ' ', '50', ' \\\\', 'text', '{', ' minutes', '}', ' =', ' \\\\', 'frac', '{', '50', '}{', '60', '}', ' \\\\', 'text', '{', ' hours', '}', ' =', ' \\\\', 'frac', '{', '5', '}{', '6', '}', ' \\\\', 'text', '{', ' hours', '}\\n', '  ', ' \\\\', ']\\n\\n', '3', '.', ' **', 'Calculate', ' earnings', ':', '**\\n', '  ', ' \\\\', '[\\n', '  ', ' \\\\', 'text', '{', 'E', 'arnings', '}', ' =', ' \\\\', 'text', '{', 'Hour', 'ly', ' rate', '}', ' \\\\', 'times', ' \\\\', 'text', '{', 'Time', ' worked', '}', ' =', ' ', '12', ' \\\\', 'times', ' \\\\', 'frac', '{', '5', '}{', '6', '}', ' =', ' ', '10', '\\n', '  ', ' \\\\', ']\\n\\n', '4', '.', ' **', 'Final', ' Answer', ':', '**\\n', '  ', ' \\\\', '[\\n', '  ', ' \\\\', 'boxed', '{', '10', '}\\n', '  ', ' \\\\', ']', '<|eot_id|>']\n",
      "decoded_assistant: ['First', ',', ' I', ' need', ' to', ' determine', ' how', ' much', ' W', 'eng', ' earned', ' for', ' babys', 'itting', '.', ' She', ' earns', ' $', '12', ' per', ' hour', ',', ' and', ' she', ' worked', ' for', ' ', '50', ' minutes', '.\\n\\n', 'Since', ' her', ' pay', ' rate', ' is', ' given', ' in', ' hours', ',', ' I', ' should', ' convert', ' the', ' ', '50', ' minutes', ' into', ' hours', '.', ' There', ' are', ' ', '60', ' minutes', ' in', ' an', ' hour', ',', ' so', ' ', '50', ' minutes', ' is', ' ', '50', '/', '60', ' hours', ',', ' which', ' simpl', 'ifies', ' to', ' ', '5', '/', '6', ' of', ' an', ' hour', '.\\n\\n', 'Next', ',', ' I', \"'ll\", ' calculate', ' her', ' earnings', ' by', ' multiplying', ' her', ' hourly', ' rate', ' by', ' the', ' number', ' of', ' hours', ' she', ' worked', '.', ' So', ',', ' $', '12', ' multiplied', ' by', ' ', '5', '/', '6', ' equals', ' $', '10', '.\\n\\n', 'Therefore', ',', ' W', 'eng', ' earned', ' $', '10', ' for', ' babys', 'itting', ' yesterday', '.\\n', '</', 'think', '>\\n\\n', '1', '.', ' **', 'D', 'etermine', ' the', ' hourly', ' rate', ' and', ' time', ' worked', ':', '**\\n', '  ', ' -', ' **', 'Hour', 'ly', ' rate', ':**', ' \\\\$', '12', ' per', ' hour', '\\n', '  ', ' -', ' **', 'Time', ' worked', ':**', ' ', '50', ' minutes', '\\n\\n', '2', '.', ' **', 'Convert', ' minutes', ' to', ' hours', ':', '**\\n', '  ', ' \\\\', '[\\n', '  ', ' ', '50', ' \\\\', 'text', '{', ' minutes', '}', ' =', ' \\\\', 'frac', '{', '50', '}{', '60', '}', ' \\\\', 'text', '{', ' hours', '}', ' =', ' \\\\', 'frac', '{', '5', '}{', '6', '}', ' \\\\', 'text', '{', ' hours', '}\\n', '  ', ' \\\\', ']\\n\\n', '3', '.', ' **', 'Calculate', ' earnings', ':', '**\\n', '  ', ' \\\\', '[\\n', '  ', ' \\\\', 'text', '{', 'E', 'arnings', '}', ' =', ' \\\\', 'text', '{', 'Hour', 'ly', ' rate', '}', ' \\\\', 'times', ' \\\\', 'text', '{', 'Time', ' worked', '}', ' =', ' ', '12', ' \\\\', 'times', ' \\\\', 'frac', '{', '5', '}{', '6', '}', ' =', ' ', '10', '\\n', '  ', ' \\\\', ']\\n\\n', '4', '.', ' **', 'Final', ' Answer', ':', '**\\n', '  ', ' \\\\', '[\\n', '  ', ' \\\\', 'boxed', '{', '10', '}\\n', '  ', ' \\\\', ']']\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, tensor(5451), tensor(11), tensor(358), tensor(1205), tensor(311), tensor(8417), tensor(1268), tensor(1790), tensor(468), tensor(833), tensor(15662), tensor(369), tensor(71683), tensor(15154), tensor(13), tensor(3005), tensor(64859), tensor(400), tensor(717), tensor(824), tensor(6596), tensor(11), tensor(323), tensor(1364), tensor(6575), tensor(369), tensor(220), tensor(1135), tensor(4520), tensor(382), tensor(12834), tensor(1077), tensor(2343), tensor(4478), tensor(374), tensor(2728), tensor(304), tensor(4207), tensor(11), tensor(358), tensor(1288), tensor(5625), tensor(279), tensor(220), tensor(1135), tensor(4520), tensor(1139), tensor(4207), tensor(13), tensor(2684), tensor(527), tensor(220), tensor(1399), tensor(4520), tensor(304), tensor(459), tensor(6596), tensor(11), tensor(779), tensor(220), tensor(1135), tensor(4520), tensor(374), tensor(220), tensor(1135), tensor(14), tensor(1399), tensor(4207), tensor(11), tensor(902), tensor(15858), tensor(9803), tensor(311), tensor(220), tensor(20), tensor(14), tensor(21), tensor(315), tensor(459), tensor(6596), tensor(382), tensor(5971), tensor(11), tensor(358), tensor(3358), tensor(11294), tensor(1077), tensor(24608), tensor(555), tensor(85292), tensor(1077), tensor(47729), tensor(4478), tensor(555), tensor(279), tensor(1396), tensor(315), tensor(4207), tensor(1364), tensor(6575), tensor(13), tensor(2100), tensor(11), tensor(400), tensor(717), tensor(56016), tensor(555), tensor(220), tensor(20), tensor(14), tensor(21), tensor(17239), tensor(400), tensor(605), tensor(382), tensor(55915), tensor(11), tensor(468), tensor(833), tensor(15662), tensor(400), tensor(605), tensor(369), tensor(71683), tensor(15154), tensor(13985), tensor(627), tensor(524), tensor(27963), tensor(1363), tensor(16), tensor(13), tensor(3146), tensor(35), tensor(25296), tensor(279), tensor(47729), tensor(4478), tensor(323), tensor(892), tensor(6575), tensor(25), tensor(1035), tensor(256), tensor(482), tensor(3146), tensor(31354), tensor(398), tensor(4478), tensor(68063), tensor(33982), tensor(717), tensor(824), tensor(6596), tensor(198), tensor(256), tensor(482), tensor(3146), tensor(1489), tensor(6575), tensor(68063), tensor(220), tensor(1135), tensor(4520), tensor(271), tensor(17), tensor(13), tensor(3146), tensor(12281), tensor(4520), tensor(311), tensor(4207), tensor(25), tensor(1035), tensor(256), tensor(1144), tensor(9837), tensor(256), tensor(220), tensor(1135), tensor(1144), tensor(1342), tensor(90), tensor(4520), tensor(92), tensor(284), tensor(1144), tensor(38118), tensor(90), tensor(1135), tensor(15523), tensor(1399), tensor(92), tensor(1144), tensor(1342), tensor(90), tensor(4207), tensor(92), tensor(284), tensor(1144), tensor(38118), tensor(90), tensor(20), tensor(15523), tensor(21), tensor(92), tensor(1144), tensor(1342), tensor(90), tensor(4207), tensor(534), tensor(256), tensor(1144), tensor(2595), tensor(18), tensor(13), tensor(3146), tensor(48966), tensor(24608), tensor(25), tensor(1035), tensor(256), tensor(1144), tensor(9837), tensor(256), tensor(1144), tensor(1342), tensor(90), tensor(36), tensor(15202), tensor(92), tensor(284), tensor(1144), tensor(1342), tensor(90), tensor(31354), tensor(398), tensor(4478), tensor(92), tensor(1144), tensor(15487), tensor(1144), tensor(1342), tensor(90), tensor(1489), tensor(6575), tensor(92), tensor(284), tensor(220), tensor(717), tensor(1144), tensor(15487), tensor(1144), tensor(38118), tensor(90), tensor(20), tensor(15523), tensor(21), tensor(92), tensor(284), tensor(220), tensor(605), tensor(198), tensor(256), tensor(1144), tensor(2595), tensor(19), tensor(13), tensor(3146), tensor(19918), tensor(22559), tensor(25), tensor(1035), tensor(256), tensor(1144), tensor(9837), tensor(256), tensor(1144), tensor(80175), tensor(90), tensor(605), tensor(534), tensor(256), tensor(1144), tensor(60), -100]\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[1]['messages']\n",
    "formatted_chat = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False,\n",
    "                )\n",
    "tokenized_chat = tokenizer(formatted_chat, return_tensors=\"pt\", \n",
    "                        # padding=\"max_length\", \n",
    "                        max_length=MAX_LENGTH,\n",
    "                        )\n",
    "input_ids = tokenized_chat['input_ids'][0]\n",
    "attention_mask = tokenized_chat['attention_mask'][0]\n",
    "\n",
    "labels = create_masked_labels(messages, tokenizer, tokenized_chat['input_ids'][0], tokenized_chat['attention_mask'][0])\n",
    "print(labels)\n",
    "# print(tokenizer.decode(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a0fe5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, I need to determine how much Weng earned for babysitting. She earns $12 per hour, and she worked for 50 minutes.\n",
      "\n",
      "Since her pay rate is given in hours, I should convert the 50 minutes into hours. There are 60 minutes in an hour, so 50 minutes is 50/60 hours, which simplifies to 5/6 of an hour.\n",
      "\n",
      "Next, I'll calculate her earnings by multiplying her hourly rate by the number of hours she worked. So, $12 multiplied by 5/6 equals $10.\n",
      "\n",
      "Therefore, Weng earned $10 for babysitting yesterday.\n",
      "</think>\n",
      "\n",
      "1. **Determine the hourly rate and time worked:**\n",
      "   - **Hourly rate:** \\$12 per hour\n",
      "   - **Time worked:** 50 minutes\n",
      "\n",
      "2. **Convert minutes to hours:**\n",
      "   \\[\n",
      "   50 \\text{ minutes} = \\frac{50}{60} \\text{ hours} = \\frac{5}{6} \\text{ hours}\n",
      "   \\]\n",
      "\n",
      "3. **Calculate earnings:**\n",
      "   \\[\n",
      "   \\text{Earnings} = \\text{Hourly rate} \\times \\text{Time worked} = 12 \\times \\frac{5}{6} = 10\n",
      "   \\]\n",
      "\n",
      "4. **Final Answer:**\n",
      "   \\[\n",
      "   \\boxed{10}\n",
      "   \\]\n"
     ]
    }
   ],
   "source": [
    "labels_ids = [label_id for label_id in labels if label_id != -100]\n",
    "print(tokenizer.decode(labels_ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf7d316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 05 Jan 2026\\n\\nPlease solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\\\boxed{}.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nStep 1: April = 48\\n\\nStep 2: May = 48/2 = 24\\n\\nStep 3: Total = 48 + 24 = \\\\boxed{72}<|eot_id|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_chat['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa0ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
