{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b159afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a47cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../datasets/gsm8k_step_jepa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657a71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=data_file)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ff52de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
       " 'ground_truth': '72',\n",
       " 'total_steps': 7,\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': 'Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\\\boxed{}.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"First, I note that Natalia sold 48 clips in April.\\n\\nIn May, she sold half as many clips as she did in April, which is 24 clips.\\n\\nTo find the total number of clips sold over both months, I add the clips sold in April and May: 48 + 24 = 72.\\n\\nTherefore, Natalia sold a total of 72 clips in April and May.\\n</think>\\n\\n**Step 1:** Determine the number of clips Natalia sold in April.\\n\\\\[\\n\\\\text{Clips sold in April} = 48\\n\\\\]\\n\\n**Step 2:** Calculate the number of clips sold in May, which is half of April's sales.\\n\\\\[\\n\\\\text{Clips sold in May} = \\\\frac{48}{2} = 24\\n\\\\]\\n\\n**Step 3:** Find the total number of clips sold in both months by adding April and May sales.\\n\\\\[\\n\\\\text{Total clips sold} = 48 + 24 = \\\\boxed{72}\\n\\\\]\"}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indices = []\n",
    "for i, sample in enumerate(dataset):\n",
    "    for msg in sample.get(\"messages\", []):\n",
    "        if msg.get(\"role\") == \"user\" and \"\\n\\n\" in (msg.get(\"content\") or \"\"):\n",
    "            bad_indices.append(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac7146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# login to huggingface\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ee3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from gsm8k.py - Full code for label masking\n",
    "def load_and_prepare_dataset(data_file, tokenizer,\n",
    "                             max_length=2048, debug=0):\n",
    "    \"\"\"Load JSONL dataset and format for training with proper label masking\"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset('json', data_files=data_file)['train']\n",
    "    if  torch.cuda.current_device() == 0:\n",
    "        print(f\"Loaded {len(dataset)} examples from {data_file}\")\n",
    "    \n",
    "    def tokenize_conversations(examples):\n",
    "        \"\"\"Tokenize conversations and mask input tokens properly\"\"\"\n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        attention_mask_list = []\n",
    "\n",
    "        for msg_idx, messages in enumerate(examples['messages']):\n",
    "            # Apply chat template if available, otherwise format manually\n",
    "            formatted_chat = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "            \n",
    "            # Tokenize the formatted conversation with padding to max_length\n",
    "            tokenized = tokenizer(\n",
    "                formatted_chat,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",  # Pad to max_length for consistent tensor shapes\n",
    "                return_tensors=None\n",
    "            )\n",
    "            \n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            \n",
    "            # Create labels with proper masking\n",
    "            labels = create_masked_labels(messages, tokenizer, input_ids, attention_mask, formatted_chat=formatted_chat)\n",
    "            \n",
    "            input_ids_list.append(input_ids)\n",
    "            labels_list.append(labels)\n",
    "            attention_mask_list.append(attention_mask)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids_list,\n",
    "            \"labels\": labels_list,\n",
    "            \"attention_mask\": attention_mask_list,\n",
    "        }\n",
    "    \n",
    "    def create_masked_labels(messages, tokenizer, input_ids, attention_mask, formatted_chat=None):\n",
    "        \"\"\"Create labels with input tokens masked (-100)\"\"\"\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # Mask padding tokens in labels\n",
    "        for i, mask in enumerate(attention_mask):\n",
    "            if mask == 0:  # Padding token\n",
    "                labels[i] = -100\n",
    "        \n",
    "        # Find assistant responses and unmask only those tokens\n",
    "        for msg in messages:\n",
    "            if msg['role'] == 'assistant':\n",
    "                assistant_content = msg['content']\n",
    "                \n",
    "                # Find assistant marker in decoded text to locate assistant content accurately\n",
    "                assistant_marker = '<|start_header_id|>assistant<|end_header_id|>'\n",
    "                decoded_full = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "                \n",
    "                # Find assistant marker position in decoded text\n",
    "                if assistant_marker not in decoded_full:\n",
    "                    # If marker not found, skip this assistant message\n",
    "                    continue\n",
    "                \n",
    "                marker_pos_in_decoded = decoded_full.find(assistant_marker)\n",
    "                # Assistant content starts after the marker\n",
    "                assistant_text_start = marker_pos_in_decoded + len(assistant_marker)\n",
    "                \n",
    "                # Map assistant marker position to token position using incremental decode\n",
    "                marker_token_pos = None\n",
    "                for i in range(len(input_ids)):\n",
    "                    if attention_mask[i] == 0:  # Skip padding\n",
    "                        continue\n",
    "                    decoded_so_far = tokenizer.decode(input_ids[:i+1], skip_special_tokens=False)\n",
    "                    if len(decoded_so_far) > marker_pos_in_decoded + len(assistant_marker):\n",
    "                        # Found the token right after assistant marker\n",
    "                        marker_token_pos = i\n",
    "                        break\n",
    "                \n",
    "                if marker_token_pos is None:\n",
    "                    # Couldn't find marker position, skip\n",
    "                    continue\n",
    "                \n",
    "                # Now find assistant content starting from after the marker\n",
    "                # Decode from marker_token_pos onwards to find where assistant_content starts\n",
    "                assistant_start_token = None\n",
    "                assistant_end_token = None\n",
    "                \n",
    "                # Decode incrementally from marker to find assistant content\n",
    "                text_after_marker = \"\"\n",
    "                for i in range(marker_token_pos, len(input_ids)):\n",
    "                    if attention_mask[i] == 0:  # Skip padding\n",
    "                        continue\n",
    "                    piece = tokenizer.decode([input_ids[i]], skip_special_tokens=False)\n",
    "                    text_after_marker += piece\n",
    "                    \n",
    "                    # Check if we've found the start of assistant content\n",
    "                    if assistant_start_token is None and assistant_content in text_after_marker:\n",
    "                        # Found start of assistant content\n",
    "                        assistant_start_token = i\n",
    "                    \n",
    "                    # Check if we've found the full assistant content\n",
    "                    if assistant_start_token is not None:\n",
    "                        # Check if we have the full content\n",
    "                        if len(text_after_marker) >= text_after_marker.find(assistant_content) + len(assistant_content):\n",
    "                            assistant_end_token = i\n",
    "                            # Don't break - continue to find last non-padding token\n",
    "                    \n",
    "                    # Keep text buffer reasonable size\n",
    "                    if len(text_after_marker) > len(assistant_content) * 2:\n",
    "                        text_after_marker = text_after_marker[-len(assistant_content):]\n",
    "                \n",
    "                # Find last non-padding token (in case assistant content was truncated)\n",
    "                last_non_padding = None\n",
    "                for i in range(len(input_ids) - 1, -1, -1):\n",
    "                    if attention_mask[i] == 1:\n",
    "                        last_non_padding = i\n",
    "                        break\n",
    "                \n",
    "                # Unmask assistant tokens: from assistant_start_token to last_non_padding\n",
    "                # (This covers the full assistant content, even if truncated)\n",
    "                if assistant_start_token is not None and last_non_padding is not None:\n",
    "                    for j in range(assistant_start_token, last_non_padding + 1):\n",
    "                        if attention_mask[j] == 1:  # Only unmask non-padding tokens\n",
    "                            labels[j] = input_ids[j]\n",
    "                \n",
    "                if debug == 4:\n",
    "                    exit(0)\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_conversations,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "\n",
    "def tokenize_text(tokenizer, text, max_length=2048, pad_to_max=True):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\" if pad_to_max else False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "\n",
    "def build_full_text(tokenizer, messages):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "\n",
    "def build_prefix_up_to_assistant_content(tokenizer, messages, assistant_index):\n",
    "    \"\"\"\n",
    "    Builds a prefix string that ends *exactly after* the assistant header + its mandatory '\\n\\n',\n",
    "    but BEFORE assistant content begins.\n",
    "    \"\"\"\n",
    "    before = messages[:assistant_index]  # everything before assistant message\n",
    "    assistant_header_only = [{\"role\": \"assistant\", \"content\": \"\"}]  # assistant header\n",
    "\n",
    "    prefix = tokenizer.apply_chat_template(\n",
    "        before + assistant_header_only,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # IMPORTANT:\n",
    "    # apply_chat_template produces:\n",
    "    # <assistant header>\\n\\n<|eot_id|>\n",
    "    # but we want to stop BEFORE <|eot_id|> because that's after assistant content would end.\n",
    "    #\n",
    "    # So remove the trailing \"<|eot_id|>\" if present, and keep the '\\n\\n' that marks content start.\n",
    "    if prefix.endswith(\"<|eot_id|>\"):\n",
    "        prefix = prefix[:-len(\"<|eot_id|>\")]\n",
    "\n",
    "    return prefix\n",
    "\n",
    "\n",
    "def compute_assistant_spans(tokenizer, full_text, messages, max_length=2048):\n",
    "    \"\"\"\n",
    "    Returns list of (start_idx, end_idx) spans in token space for assistant CONTENT ONLY.\n",
    "    Each span excludes assistant headers and includes the content tokens only.\n",
    "    \"\"\"\n",
    "\n",
    "    full = tokenize_text(tokenizer, full_text, max_length=max_length, pad_to_max=True)\n",
    "    full_ids = full[\"input_ids\"]\n",
    "    full_mask = full[\"attention_mask\"]\n",
    "\n",
    "    last_non_pad = max(i for i, m in enumerate(full_mask) if m == 1)\n",
    "\n",
    "    spans = []\n",
    "    full_len = last_non_pad + 1\n",
    "\n",
    "    # We find each assistant message span by:\n",
    "    # start = token length of prefix ending at assistant content start\n",
    "    # end   = token length of prefix ending at assistant content end (right before next role header)\n",
    "    #\n",
    "    # We'll do this by constructing prefixes incrementally.\n",
    "\n",
    "    for idx, msg in enumerate(messages):\n",
    "        if msg[\"role\"] != \"assistant\":\n",
    "            continue\n",
    "\n",
    "        # Prefix right before assistant content begins\n",
    "        prefix_start_text = build_prefix_up_to_assistant_content(tokenizer, messages, idx)\n",
    "        prefix_start_ids = tokenize_text(tokenizer, prefix_start_text, max_length=max_length, pad_to_max=False)[\"input_ids\"]\n",
    "        start = len(prefix_start_ids)\n",
    "\n",
    "        # Prefix up through assistant content (i.e., include this assistant fully)\n",
    "        prefix_end_messages = messages[:idx+1]\n",
    "        prefix_end_text = tokenizer.apply_chat_template(\n",
    "            prefix_end_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        prefix_end_ids = tokenize_text(tokenizer, prefix_end_text, max_length=max_length, pad_to_max=False)[\"input_ids\"]\n",
    "        end = len(prefix_end_ids)\n",
    "\n",
    "        # In full conversation, assistant span is [start, end) BUT must exclude the final <|eot_id|>\n",
    "        # Usually the assistant message ends with <|eot_id|>. We want to exclude it.\n",
    "        #\n",
    "        # So shrink end by 1 if last token decodes to <|eot_id|>\n",
    "        if end > start:\n",
    "            last_tok = tokenizer.decode([prefix_end_ids[-1]], skip_special_tokens=False)\n",
    "            if last_tok == \"<|eot_id|>\":\n",
    "                end -= 1\n",
    "\n",
    "        # Clamp to avoid exceeding truncation\n",
    "        start = min(start, full_len)\n",
    "        end = min(end, full_len)\n",
    "\n",
    "        if start < end:\n",
    "            spans.append((start, end))\n",
    "\n",
    "    return full_ids, full_mask, spans, last_non_pad\n",
    "\n",
    "\n",
    "def create_masked_labels(messages, tokenizer, max_length=2048):\n",
    "    full_text = build_full_text(tokenizer, messages)\n",
    "    input_ids, attention_mask, spans, last_non_pad = compute_assistant_spans(\n",
    "        tokenizer, full_text, messages, max_length=max_length\n",
    "    )\n",
    "\n",
    "    labels = [-100] * len(input_ids)\n",
    "\n",
    "    # Unmask each assistant content span\n",
    "    for start, end in spans:\n",
    "        for i in range(start, end):\n",
    "            labels[i] = input_ids[i]\n",
    "\n",
    "    # Mask padding (already -100 but keep safe)\n",
    "    for i in range(last_non_pad + 1, len(labels)):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return input_ids, attention_mask, labels, spans, last_non_pad, full_text\n",
    "\n",
    "\n",
    "def pretty_print_debug(tokenizer, input_ids, labels, spans, last_non_pad, window=50):\n",
    "    \"\"\"\n",
    "    Print tokens around each assistant span.\n",
    "    \"\"\"\n",
    "    for span_id, (start, end) in enumerate(spans):\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(f\"ASSISTANT SPAN {span_id}: start={start}, end={end} (len={end-start})\")\n",
    "        print(f\"LAST NON-PAD IDX: {last_non_pad}\")\n",
    "        print(\"=\" * 90)\n",
    "\n",
    "        s = max(0, start - window)\n",
    "        e = min(len(input_ids), start + window)\n",
    "\n",
    "        print(\"Index | Masked? | Token (decoded)\")\n",
    "        print(\"-\" * 90)\n",
    "\n",
    "        for i in range(s, e):\n",
    "            tok = tokenizer.decode([input_ids[i]], skip_special_tokens=False)\n",
    "            masked = (labels[i] == -100)\n",
    "            print(f\"{i:5d} | {str(masked):6s} | {repr(tok)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Please solve the problem step by step (separate steps with double newlines), \"\n",
    "                       \"but keep it short and put your final answer (do not include any other text or units) \"\n",
    "                       \"within \\\\boxed{}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \"\n",
    "                       \"How many clips did Natalia sell altogether in April and May?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Step 1: April = 48\\n\\nStep 2: May = 48/2 = 24\\n\\nStep 3: Total = 48 + 24 = \\\\boxed{72}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    input_ids, attention_mask, labels, spans, last_non_pad, full_text = create_masked_labels(\n",
    "        messages, tokenizer, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # Sanity check\n",
    "    total_tokens = len(input_ids)\n",
    "    masked = sum(1 for x in labels if x == -100)\n",
    "    unmasked = total_tokens - masked\n",
    "\n",
    "    print(\"\\nFULL CHAT TEXT (truncated to 600 chars):\")\n",
    "    print(full_text[:600])\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    print(f\"\\nTotal tokens:   {total_tokens}\")\n",
    "    print(f\"Masked tokens:  {masked}\")\n",
    "    print(f\"Unmasked tokens:{unmasked}\")\n",
    "    print(f\"Assistant spans: {spans}\")\n",
    "\n",
    "    # Validate that assistant header tokens are masked:\n",
    "    # We'll assert that the token just before each span start is masked (usually '\\n\\n' or header tokens)\n",
    "    for (start, end) in spans:\n",
    "        if start > 0:\n",
    "            assert labels[start - 1] == -100, \"❌ Assistant header/newline token incorrectly unmasked!\"\n",
    "\n",
    "    # Validate that each span contains unmasked tokens\n",
    "    for (start, end) in spans:\n",
    "        assert any(labels[i] != -100 for i in range(start, end)), \"❌ Span has no unmasked tokens!\"\n",
    "\n",
    "    # Validate padding masked\n",
    "    assert all(labels[i] == -100 for i in range(last_non_pad + 1, len(labels))), \"❌ Padding tokens unmasked!\"\n",
    "\n",
    "    print(\"\\n✅ All masking sanity checks passed!\\n\")\n",
    "\n",
    "    pretty_print_debug(tokenizer, input_ids, labels, spans, last_non_pad, window=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d07b1fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FULL CHAT TEXT (truncated to 600 chars):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 05 Jan 2026\n",
      "\n",
      "Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Step 1: April = 48\n",
      "\n",
      "Step 2: May = \n",
      "\n",
      "================================================================================\n",
      "\n",
      "Total tokens:   2048\n",
      "Masked tokens:  2008\n",
      "Unmasked tokens:40\n",
      "Assistant spans: [(113, 153)]\n",
      "\n",
      "✅ All masking sanity checks passed!\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "ASSISTANT SPAN 0: start=113, end=153 (len=40)\n",
      "LAST NON-PAD IDX: 153\n",
      "==========================================================================================\n",
      "Index | Masked? | Token (decoded)\n",
      "------------------------------------------------------------------------------------------\n",
      "   78 | True   | ' of'\n",
      "   79 | True   | ' her'\n",
      "   80 | True   | ' friends'\n",
      "   81 | True   | ' in'\n",
      "   82 | True   | ' April'\n",
      "   83 | True   | ','\n",
      "   84 | True   | ' and'\n",
      "   85 | True   | ' then'\n",
      "   86 | True   | ' she'\n",
      "   87 | True   | ' sold'\n",
      "   88 | True   | ' half'\n",
      "   89 | True   | ' as'\n",
      "   90 | True   | ' many'\n",
      "   91 | True   | ' clips'\n",
      "   92 | True   | ' in'\n",
      "   93 | True   | ' May'\n",
      "   94 | True   | '.'\n",
      "   95 | True   | ' How'\n",
      "   96 | True   | ' many'\n",
      "   97 | True   | ' clips'\n",
      "   98 | True   | ' did'\n",
      "   99 | True   | ' Natal'\n",
      "  100 | True   | 'ia'\n",
      "  101 | True   | ' sell'\n",
      "  102 | True   | ' altogether'\n",
      "  103 | True   | ' in'\n",
      "  104 | True   | ' April'\n",
      "  105 | True   | ' and'\n",
      "  106 | True   | ' May'\n",
      "  107 | True   | '?'\n",
      "  108 | True   | '<|eot_id|>'\n",
      "  109 | True   | '<|start_header_id|>'\n",
      "  110 | True   | 'assistant'\n",
      "  111 | True   | '<|end_header_id|>'\n",
      "  112 | True   | '\\n\\n'\n",
      "  113 | False  | 'Step'\n",
      "  114 | False  | ' '\n",
      "  115 | False  | '1'\n",
      "  116 | False  | ':'\n",
      "  117 | False  | ' April'\n",
      "  118 | False  | ' ='\n",
      "  119 | False  | ' '\n",
      "  120 | False  | '48'\n",
      "  121 | False  | '\\n\\n'\n",
      "  122 | False  | 'Step'\n",
      "  123 | False  | ' '\n",
      "  124 | False  | '2'\n",
      "  125 | False  | ':'\n",
      "  126 | False  | ' May'\n",
      "  127 | False  | ' ='\n",
      "  128 | False  | ' '\n",
      "  129 | False  | '48'\n",
      "  130 | False  | '/'\n",
      "  131 | False  | '2'\n",
      "  132 | False  | ' ='\n",
      "  133 | False  | ' '\n",
      "  134 | False  | '24'\n",
      "  135 | False  | '\\n\\n'\n",
      "  136 | False  | 'Step'\n",
      "  137 | False  | ' '\n",
      "  138 | False  | '3'\n",
      "  139 | False  | ':'\n",
      "  140 | False  | ' Total'\n",
      "  141 | False  | ' ='\n",
      "  142 | False  | ' '\n",
      "  143 | False  | '48'\n",
      "  144 | False  | ' +'\n",
      "  145 | False  | ' '\n",
      "  146 | False  | '24'\n",
      "  147 | False  | ' ='\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "880a921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e5d59a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 05 Jan 2026\n",
      "\n",
      "Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "First, I note that Natalia sold 48 clips in April.\n",
      "\n",
      "In May, she sold half as many clips as she did in April, which is 24 clips.\n",
      "\n",
      "To find the total number of clips sold over both months, I add the clips sold in April and May: 48 + 24 = 72.\n",
      "\n",
      "Therefore, Natalia sold a total of 72 clips in April and May.\n",
      "</think>\n",
      "\n",
      "**Step 1:** Determine the number of clips Natalia sold in April.\n",
      "\\[\n",
      "\\text{Clips sold in April} = 48\n",
      "\\]\n",
      "\n",
      "**Step 2:** Calculate the number of clips sold in May, which is half of April's sales.\n",
      "\\[\n",
      "\\text{Clips sold in May} = \\frac{48}{2} = 24\n",
      "\\]\n",
      "\n",
      "**Step 3:** Find the total number of clips sold in both months by adding April and May sales.\n",
      "\\[\n",
      "\\text{Total clips sold} = 48 + 24 = \\boxed{72}\n",
      "\\]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(\n",
    "                    dataset[0]['messages'],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False,\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bccc6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54b42ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_chat = tokenizer.apply_chat_template(\n",
    "                    dataset[0]['messages'],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9e2a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 04 Jan 2026\n",
      "\n",
      "Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "First, I note that Natalia sold 48 clips in April.\n",
      "\n",
      "In May, she sold half as many clips as she did in April, which is 24 clips.\n",
      "\n",
      "To find the total number of clips sold over both months, I add the clips sold in April and May: 48 + 24 = 72.\n",
      "\n",
      "Therefore, Natalia sold a total of 72 clips in April and May.\n",
      "</think>\n",
      "\n",
      "**Step 1:** Determine the number of clips Natalia sold in April.\n",
      "\\[\n",
      "\\text{Clips sold in April} = 48\n",
      "\\]\n",
      "\n",
      "**Step 2:** Calculate the number of clips sold in May, which is half of April's sales.\n",
      "\\[\n",
      "\\text{Clips sold in May} = \\frac{48}{2} = 24\n",
      "\\]\n",
      "\n",
      "**Step 3:** Find the total number of clips sold in both months by adding April and May sales.\n",
      "\\[\n",
      "\\text{Total clips sold} = 48 + 24 = \\boxed{72}\n",
      "\\]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatted_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c959ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 04 Jan 2026\n",
      "\n",
      "Please solve the problem step by step (separate steps with double newlines), but keep it short and put your final answer (do not include any other text or units) within \\boxed{}.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "First, I note that Natalia sold 48 clips in April.\n",
      "\n",
      "In May, she sold half as many clips as she did in April, which is 24 clips.\n",
      "\n",
      "To find the total number of clips sold over both months, I add the clips sold in April and May: 48 + 24 = 72.\n",
      "\n",
      "Therefore, Natalia sold a total of 72 clips in April and May.\n",
      "</think>\n",
      "\n",
      "**Step 1:** Determine the number of clips Natalia sold in April.\n",
      "\\[\n",
      "\\text{Clips sold in April} = 48\n",
      "\\]\n",
      "\n",
      "**Step 2:** Calculate the number of clips sold in May, which is half of April's sales.\n",
      "\\[\n",
      "\\text{Clips sold in May} = \\frac{48}{2} = 24\n",
      "\\]\n",
      "\n",
      "**Step 3:** Find the total number of clips sold in both months by adding April and May sales.\n",
      "\\[\n",
      "\\text{Total clips sold} = 48 + 24 = \\boxed{72}\n",
      "\\]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(formatted_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadedfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
